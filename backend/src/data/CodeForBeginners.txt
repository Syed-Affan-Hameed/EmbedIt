//Endpoints for Step By Step Comprehension.
import { Request, Response } from "express";
import { OpenAI } from "openai"; // Adjust the import based on your actual library
import dotenv from "dotenv";
import fs from "fs";
import { MessageContent } from "openai/resources/beta/threads/messages.mjs";

dotenv.config();

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

let assistantId: string = "";//asst_uCbpp0XvOkNM2VH0I03rFIcd
let threadId: string = "";//thread_8x3gfVWpT699k17Xt57GfYew

export const createAssistant = async (req: Request, res: Response) => {
  try {
    const assistant = await openai.beta.assistants.create({
      name: "Embedding Model Selection Assistant",
      instructions:
        "You are an expert in Selecting Embedding Models. Use your knowledge base to answer questions about selecting the right Embedding models for the development of LLM driven AI-Applications.",
      model: "gpt-3.5-turbo",
      tools: [{ type: "file_search" }],
    });
    assistantId = assistant.id;
    console.log(assistant);
    res
      .status(200)
      .json({
        message: "Step 1] Complete: Assistant created successfully",
        assistant,
      });
  } catch (error: any) {
    res.status(500).json({ error: error.message });
  }
};

export const createVectorStoreAndAddDocuments = async (
  req: Request,
  res: Response
) => {
  try {
    const fileStreams = ["./backend/src/data/howtoselectembeddingmodelsample.pdf"].map((path) =>
      fs.createReadStream(path)
    );

    // Create a vector store including our two files.
    let vectorStore = await openai.beta.vectorStores.create({
      name: "Embedding Models information",
    });
    const uploadAndPollResult =
      await openai.beta.vectorStores.fileBatches.uploadAndPoll(vectorStore.id, {
        files: fileStreams, // Wrap the fileStreams array in an object with the 'files' key
      });

    await openai.beta.assistants.update(assistantId, {
      tool_resources: { file_search: { vector_store_ids: [vectorStore.id] } },
    });

    res
      .status(200)
      .json({
        message:
          "Step 2] Complete: Vector Store created and documents added successfully",
        vectorStore,
        uploadAndPollResult,
      });
  } catch (error: any) {
    res.status(500).json({ error: error.message });
  }
};

export const createThread = async (req: Request, res: Response) => {
  try {
    const thread = await openai.beta.threads.create({
      messages: [
        {
          role: "user",
          content:
            "Can you tell me something about the importance of embeddings in AI?",
        },
      ],
    });
    // setting the thread id for global access.
    threadId = thread.id;

    res
      .status(200)
      .json({
        messages: "Step 3] Successful created a thread" + "id:" + threadId,
      });
  } catch (error: any) {
    res.status(500).json({ error: error.message });
  }
};

export const createRun = async (req: Request, res: Response) => {
  try {
    const { text, citations } = await runAssistantWithCitations(
      threadId,
      assistantId
    );

    res.status(200).json({
      message: "Run created successfully",
      text,
      citations,
    });
  } catch (error: any) {
    res.status(500).json({ error: error.message });
  }
};

export const addFollowUp = async (req: Request, res: Response) => {
  const  followUpQuestion  = req.body.question; // Expect followUpQuestion in the request body

  try {
    const t=threadId;
    if (!threadId) {
      return res
        .status(400)
        .json({
          error: "Thread ID is not initialized. Create a thread first.",
        });
    }

    // Add the follow-up question to the thread
    await openai.beta.threads.messages.create(threadId, {
      role: "user",
      content: followUpQuestion,
    });

    // Run the assistant and fetch its response
    const responseText = await runAssistantWithCitations(threadId, assistantId);

    res.status(200).json({
      message: "Follow-up question added and assistant responded successfully.",
      followUpQuestion,
      response: responseText,
      citations: responseText.citations,
    });
  } catch (error: any) {
    res.status(500).json({ error: error.message });
  }
};

const runAssistantWithCitations = async (
  threadId: string,
  assistantId: string
) => {
  try {
    const run = await openai.beta.threads.runs.createAndPoll(threadId, {
      assistant_id: assistantId,
    });

    const messages = await openai.beta.threads.messages.list(threadId, {
      run_id: run.id,
    });

    // Extract the latest message from the assistant
    const lastMessage = messages.data.pop();
    if (lastMessage?.content[0]?.type === "text") {
      const { text } = lastMessage.content[0];
      const { annotations } = text;
      const citations: string[] = [];

      let index = 0;
      for (let annotation of annotations) {
        text.value = text.value.replace(annotation.text, `[${index}]`);
        //@ts-ignore
        const { file_citation } = annotation;
        if (file_citation) {
          const citedFile = await openai.files.retrieve(file_citation.file_id);
          citations.push(`[${index}] ${citedFile.filename}`);
        }
        index++;
      }

      return { text: text.value, citations };
    }

    return { text: "No response received.", citations: [] };
  } catch (error: any) {
    throw new Error(`Failed to run assistant: ${error.message}`);
  }
};
